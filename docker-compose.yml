version: '3'

services:
  namenode:
    image: apache/hadoop:3.3.6
    container_name: namenode
    hostname: namenode
    user: root
    environment:
      - HADOOP_HOME=/opt/hadoop
    volumes:
      - ./hdfs/hadoop_config:/opt/hadoop/etc/hadoop
      - ./hdfs/start-hdfs.sh:/start-hdfs.sh
      - ./hdfs/shared_data:/shared_data
    ports:
      - "9870:9870"
    command: [ "/bin/bash", "/start-hdfs.sh" ]
    networks:
      - sabd_network

  datanode1:
    image: apache/hadoop:3.3.6
    container_name: datanode1
    hostname: datanode1
    user: root
    environment:
      - HADOOP_HOME=/opt/hadoop
    volumes:
      - ./hdfs/hadoop_config:/opt/hadoop/etc/hadoop
      - ./hdfs/init-datanode.sh:/init-datanode.sh
    ports:
      - "9864:9864"
    depends_on:
      - namenode
    command: [ "/bin/bash", "/init-datanode.sh" ]
    networks:
      - sabd_network

  #datanode2:
    #image: apache/hadoop:3.3.6
    #container_name: datanode2
    #hostname: datanode2
    #user: root
    #environment:
     # - HADOOP_HOME=/opt/hadoop
    #volumes:
      #- ./hdfs/hadoop_config:/opt/hadoop/etc/hadoop
     # - ./hdfs/init-datanode.sh:/init-datanode.sh
    #ports:
     # - "9865:9864"
    #depends_on:
     # - namenode
    #command: [ "/bin/bash", "/init-datanode.sh" ]
    #networks:
      #- sabd_network
        
  #datanode3:
   # image: apache/hadoop:3.3.6
    #container_name: datanode3
    #hostname: datanode3
    #user: root
    #environment:
     # - HADOOP_HOME=/opt/hadoop
    #volumes:
      #- ./hdfs/hadoop_config:/opt/hadoop/etc/hadoop
      #- ./hdfs/init-datanode.sh:/init-datanode.sh
    #ports:
     # - "9866:9864"
    #depends_on:
     # - namenode
    #command: [ "/bin/bash", "/init-datanode.sh" ]
    #networks:
     # - sabd_network
        
  nifi:
    image: apache/nifi:1.24.0
    container_name: nifi
    hostname: nifi
    ports:
      - "8080:8080"
    environment:
      - NIFI_WEB_HTTP_PORT=8080
      - NIFI_REMOTE_INPUT_SECURE=false
      - NIFI_HADOOP_CONFIGURATION_RESOURCES=/opt/nifi/nifi-current/conf/core-site.xml,/opt/nifi/nifi-current/conf/hdfs-site.xml
    volumes:
     - ./hdfs/hadoop_config/core-site.xml:/opt/nifi/nifi-current/conf/core-site.xml
     - ./hdfs/hadoop_config/hdfs-site.xml:/opt/nifi/nifi-current/conf/hdfs-site.xml
     - ./data/raw:/opt/nifi/input_data_files
     - ./nifi/data_acquisition_and_ingestion_flow.xml:/opt/nifi/nifi-current/conf/templates/data_acquisition_and_ingestion_flow.xml
    depends_on:
      - namenode
    networks:
      - sabd_network
      
  spark-master:
    image: bitnami/spark:3.5.1
    container_name: spark-master
    hostname: spark-master
    ports:
      - "7077:7077"    # Spark master port
      - "4040:4040"    # Spark UI
    environment:
      - SPARK_MODE=master
    volumes:
      - ./hdfs/hadoop_config:/opt/hadoop/etc/hadoop
      - ./spark/scripts:/opt/spark/work-dir
    depends_on:
      - namenode
      #- datanode1
      #- datanode2
    networks:
      - sabd_network
      
  spark-worker-1:
    image: bitnami/spark:3.5.1
    container_name: spark-worker-1
    hostname: spark-worker-1
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
    depends_on:
      - spark-master
    networks:
      - sabd_network

  spark-worker-2:
    image: bitnami/spark:3.5.1
    container_name: spark-worker-2
    hostname: spark-worker-2
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
    depends_on:
      - spark-master
    networks:
      - sabd_network
      
  grafana:
    image: grafana/grafana:10.3.1
    container_name: grafana
    hostname: grafana
    ports:
      - "3000:3000"
    environment:
      - GF_INSTALL_PLUGINS=marcusolsson-csv-datasource
      - GF_PLUGIN_ALLOW_LOCAL_MODE=true
    volumes:
      - ./hdfs/shared_data:/var/lib/grafana/csv
    depends_on:
      - namenode
    networks:
      - sabd_network


networks:
  sabd_network:
    driver: bridge

version: '3'

services:
  namenode:
    image: apache/hadoop:3.3.6
    container_name: namenode
    hostname: namenode
    user: root
    environment:
      - HADOOP_HOME=/opt/hadoop
    volumes:
      - namenode_data:/opt/hadoop/data/nameNode
      - ./hdfs/hadoop_config:/opt/hadoop/etc/hadoop
      - ./hdfs/start-hdfs.sh:/start-hdfs.sh
      - ./Results/csv:/results
    ports:
      - "9870:9870"
    command: [ "/bin/bash", "/start-hdfs.sh" ]
    networks:
      - sabd_network

  datanode1:
    image: apache/hadoop:3.3.6
    container_name: datanode1
    hostname: datanode1
    user: root
    environment:
      - HADOOP_HOME=/opt/hadoop
    volumes:
      - datanode1_data:/opt/hadoop/data/dataNode
      - ./hdfs/hadoop_config:/opt/hadoop/etc/hadoop
      - ./hdfs/init-datanode.sh:/init-datanode.sh
    ports:
      - "9864:9864"
    depends_on:
      - namenode
    command: [ "/bin/bash", "/init-datanode.sh" ]
    networks:
      - sabd_network

  datanode2:
    image: apache/hadoop:3.3.6
    container_name: datanode2
    hostname: datanode2
    user: root
    environment:
      - HADOOP_HOME=/opt/hadoop
    volumes:
      - datanode2_data:/opt/hadoop/data/dataNode
      - ./hdfs/hadoop_config:/opt/hadoop/etc/hadoop
      - ./hdfs/init-datanode.sh:/init-datanode.sh
    ports:
      - "9865:9864"
    depends_on:
      - namenode
    command: [ "/bin/bash", "/init-datanode.sh" ]
    networks:
      - sabd_network
        
  #datanode3:
    #image: apache/hadoop:3.3.6
    #container_name: datanode3
    #hostname: datanode3
    #user: root
    #environment:
     # - HADOOP_HOME=/opt/hadoop
    #volumes:
      #- datanode3_data:/opt/hadoop/data/dataNode
      #- ./hdfs/hadoop_config:/opt/hadoop/etc/hadoop
      #- ./hdfs/init-datanode.sh:/init-datanode.sh
    #ports:
     # - "9866:9864"
    #depends_on:
     # - namenode
    #command: [ "/bin/bash", "/init-datanode.sh" ]
    #networks:
     # - sabd_network
        
  nifi:
    image: apache/nifi:1.24.0
    container_name: nifi
    hostname: nifi
    user: root
    ports:
      - "8080:8080"
    environment:
      - NIFI_WEB_HTTP_PORT=8080
      - NIFI_REMOTE_INPUT_SECURE=false
      - NIFI_HADOOP_CONFIGURATION_RESOURCES=/opt/nifi/nifi-current/conf/core-site.xml,/opt/nifi/nifi-current/conf/hdfs-site.xml
      - NIFI_CONTENT_REPOSITORY_ARCHIVED_MAX_USAGE_PERCENTAGE=80
      - NIFI_CONTENT_REPOSITORY_ARCHIVED_MAX_SIZE=20 GB
      - NIFI_CONTENT_REPOSITORY_ARCHIVED_MAX_RETENTION_PERIOD=24 hours
    volumes:
      - ./hdfs/hadoop_config/core-site.xml:/opt/nifi/nifi-current/conf/core-site.xml
      - ./hdfs/hadoop_config/hdfs-site.xml:/opt/nifi/nifi-current/conf/hdfs-site.xml
      - ./scripts/nifi:/opt/nifi/init-scripts
      - ./nifi/templates:/opt/nifi/nifi-current/conf/templates
    depends_on:
      - namenode
    networks:
      - sabd_network
      
  spark-master:
    build:
      context: ./spark/docker
    container_name: spark-master
    hostname: spark-master
    user: root
    ports:
      - "7077:7077"    # Spark master port
      - "4040:4040"    # Spark UI
    environment:
      - SPARK_MODE=master
    volumes:
      - ./hdfs/hadoop_config:/opt/hadoop/etc/hadoop
      - ./scripts/spark/queries:/opt/spark/work-dir
      - ./scripts/spark/export:/opt/spark/export
    depends_on:
      - namenode
      - datanode1
      - datanode2
      #- datanode3
    networks:
      - sabd_network
      
  spark-worker-1:
    image: bitnami/spark:3.5.1
    container_name: spark-worker-1
    hostname: spark-worker-1
    user: root
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
    depends_on:
      - spark-master
    networks:
      - sabd_network

  spark-worker-2:
    image: bitnami/spark:3.5.1
    container_name: spark-worker-2
    hostname: spark-worker-2
    user: root
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
    depends_on:
      - spark-master
    networks:
      - sabd_network
      
  grafana:
    image: grafana/grafana:12.0.1
    container_name: grafana
    hostname: grafana
    user: root
    ports:
      - "3000:3000"
    environment:
      - GF_INSTALL_PLUGINS=marcusolsson-csv-datasource
      - GF_PLUGIN_ALLOW_LOCAL_MODE=true
      - GF_RENDERING_SERVER_URL=http://grafana-image-renderer:8081/render
      - GF_RENDERING_CALLBACK_URL=http://grafana:3000/
      - GF_LOG_FILTERS=rendering:debug
    volumes:
      - ./Results/csv:/var/lib/grafana/csv
    depends_on:
      - namenode
      - grafana-image-renderer
    networks:
      - sabd_network
      
  grafana-image-renderer:
    image: grafana/grafana-image-renderer:3.12.0
    container_name: grafana-image-renderer
    hostname: grafana-image-renderer
    user: root
    ports:
      - "8081:8081"
    depends_on:
      - namenode
    networks:
      - sabd_network
    restart: unless-stopped
      
  redis:
    image: redis:7.2
    container_name: redis
    hostname: redis
    user: root
    ports:
      - "6379:6379"
    depends_on:
      - namenode
    networks:
      - sabd_network

networks:
  sabd_network:
    driver: bridge
    
volumes:
  namenode_data:
  datanode1_data:
  datanode2_data:

